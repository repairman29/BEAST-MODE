/**
 * Script to quantize a model
 */

const path = require('path');
const { getModelQuantization } = require('../lib/mlops/modelQuantization');
const { getMLModelIntegration } = require('../lib/mlops/mlModelIntegration');

async function main() {
  console.log('ðŸ”§ Model Quantization Script\n');

  const quantization = getModelQuantization();
  const mlIntegration = await getMLModelIntegration();
  await mlIntegration.initialize();

  if (!mlIntegration.qualityPredictor || !mlIntegration.qualityPredictor.model) {
    console.log('âŒ No model loaded to quantize');
    process.exit(1);
  }

  console.log('ðŸ“Š Quantizing model...\n');

  const modelName = mlIntegration.modelPath 
    ? path.basename(mlIntegration.modelPath, '.json')
    : 'current_model';

  const result = quantization.quantizeModel(
    mlIntegration.qualityPredictor.model,
    modelName,
    {
      bits: 8,
      preserveAccuracy: true
    }
  );

  if (result) {
    console.log('âœ… Quantization complete!\n');
    console.log('ðŸ“Š Results:');
    console.log(`   Original Size: ${(result.originalSize / 1024).toFixed(2)} KB`);
    console.log(`   Quantized Size: ${(result.quantizedSize / 1024).toFixed(2)} KB`);
    console.log(`   Compression: ${result.compressionRatio}%`);

    // Save quantized model
    const quantizedPath = path.join(
      path.dirname(mlIntegration.modelPath || '.beast-mode/models'),
      `${modelName}-quantized.json`
    );
    
    const fs = require('fs').promises;
    await fs.writeFile(quantizedPath, JSON.stringify(result.model, null, 2));
    console.log(`\nðŸ’¾ Quantized model saved to: ${quantizedPath}`);
  } else {
    console.log('âŒ Quantization failed');
    process.exit(1);
  }
}

main().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});

